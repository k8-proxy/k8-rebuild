# This is a basic workflow to help you get started with Actions

name: CI
env:
  AWS_DEFAULT_REGION: eu-west-1
# Controls when the action will run. Triggers the workflow on push or pull request
# events but only for the master branch
on:
  workflow_dispatch:
  push:
    branches: [ main, github_actions ]
    paths-ignore: 
      - 'sow-rest-*'

# A workflow run is made up of one or more jobs that can run sequentially or in parallel
jobs:
  build-ami:
    # The type of runner that the job will run on
    runs-on: ubuntu-latest
    # Steps represent a sequence of tasks that will be executed as part of the job
    steps:

    # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it  
    - uses: actions/checkout@v2

    - name: Checkout submodules
      uses: actions/checkout@v2
      with:
        repository: k8-proxy/k8-rebuild-rest-api
        path: k8-rebuild-rest-api
        submodules: true

    # - name: Run unit tests
    #   run: |
    #     sudo apt-get install -y dotnet-sdk-3.1
    #     cd k8-rebuild-rest-api/Glasswall.CloudSdk.AWS.Rebuild.Tests
    #     dotnet restore
    #     dotnet test

    # build artifact
    - name: Build AMI
      uses: operatehappy/packer-github-actions@master
      with:
        command: build
        arguments: "-color=false -on-error=cleanup -var github_sha=${{ github.sha }} -var vm_name=k8-rebuild -var region=${{ env.AWS_DEFAULT_REGION }} -var aws_access_key=${{ secrets.AWS_ACCESS_KEY }} -var aws_secret_key=${{ secrets.AWS_SECRET_ACCESS_KEY }}"
        target: packer/aws-ova.json

  deploy-ami:
    runs-on: ubuntu-latest
    needs: build-ami
    steps:
      - name: Get the current instance id
        id: get_id
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: ${{ env.AWS_DEFAULT_REGION }}
        run: |
          # instance_id="${{ steps.deploy.outputs.instance_id }}"
          instance_id=$(aws ec2 describe-instances --filters 'Name=tag:Name,Values=dev-k8-rebuild' --output text --query 'Reservations[*].Instances[*].InstanceId')
          echo ::set-output name=instance_id::$instance_id
          
      - name: Deploy AMI to dev
        id: deploy
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: ${{ env.AWS_DEFAULT_REGION }}
        run: |
          ami_id=$(aws ec2 describe-images --filters "Name=name,Values=k8-rebuild-${{ github.sha }}" --query 'Images[*].[ImageId]' --output text)
          result=$(aws ec2 run-instances --image-id $ami_id --count 1 --instance-type t2.large --key-name k8-rebuild --security-group-ids sg-0120400d5eefb0b9e --tag-specifications 'ResourceType=instance, Tags=[{Key=Name,Value=dev-k8-rebuild}, {Key=Team, Value=k8-proxy/k8-rebuild}, {Key=Owner, Value=githubactionAMIpacker}, {Key=AMI_Name, Value=k8-rebuild-${{ github.sha }}}]')
          sleep 12m
          instance_id=$(echo $result | jq -r ".Instances[0].InstanceId")
          echo "$instance_id is created."
          instance_description=$(aws ec2 describe-instances --instance-ids $instance_id)
          instance_state=$(echo $instance_description | jq -r ".Reservations[0].Instances[0].State.Name")
          echo "Instance state is $instance_state"
          if [[ "$instance_state" != "running" ]];then
              echo "EC2 instance $instance_id created from AMI has failed to start in time, terminating the instance." 
              aws ec2 terminate-instances --instance-ids $instance_id
              exit -1
          fi
          instance_ip=$(echo $instance_description | jq -r ".Reservations[0].Instances[0].PublicIpAddress")
          echo "Access the application at http://${instance_ip}"
          echo ::set-output name=instance_ip::$instance_ip
          echo ::set-output name=instance_id::$instance_id
          echo ::set-output name=ami_id::$ami_id
      - name: Checkout submodules
        uses: actions/checkout@v2
        with:
          repository: k8-proxy/vmware-scripts
          path: vmware-scripts

      - name: Run tests on the VM
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: ${{ env.AWS_DEFAULT_REGION }}
        run: |
          instance_id="${{ steps.deploy.outputs.instance_id }}"
          instance_ip="${{ steps.deploy.outputs.instance_ip }}"
          ami_id="${{ steps.deploy.outputs.ami_id }}"
          cd vmware-scripts/HealthFunctionalTests/filedrop
          npm install
          sed -i "s/3.133.161.191/$instance_ip/g" cypress/integration/filedrop.spec.js
          npm test
          echo "Tests are successfully on the new instance, terminating old instance."
          aws ec2 create-tags --resources $ami_id --tags Key=Test_Result,Value=Success
          instance_id="${{ steps.get_id.outputs.instance_id }}"
          if [[ ! -z "$instance_id" ]]; then
            aws ec2 terminate-instances --instance-ids $instance_id
          fi

      - name: Delete instance if tests fail
        if: ${{ failure() }}
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: ${{ env.AWS_DEFAULT_REGION }}
        run: |
          echo "Failed integration tests, terminating the newly deployed VM"
          instance_id="${{ steps.deploy.outputs.instance_id }}"
          aws ec2 terminate-instances --instance-ids $instance_id
          aws ec2 create-tags --resources $ami_id --tags Key=Test_Result,Value=Failed
